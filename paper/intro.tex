Database systems use memory buffers to retain some number of pages for a while after they've been read from the disk. That way, if the DBMS needs the same page again and it is in memory buffer, it can save one access to disk. The question is, when the buffer is full and we need to put another page in, which page should we drop from the buffer? One popular algorithm to determine the page to drop is Least Recently Used (LRU). With LRU, the buffer drops the page that was used least recently.

In 1993, O'Neil et al. \cite{lruk} developed an extension to LRU algorithm called LRU-K. The basic idea of LRU-K is to use last K references of the page and drop the page with least recent Kth reference in the past. With $K = 1$, the algorithm is equivalent to the classic LRU algorithm.

To better adapt LRU-K to real-world database workloads and address some of the unrealistic assumptions made by the initial algorithm, O'Neil et al. propose the Correlated Reference Period (CRP), which is a time-out period that dealt with two significant performance issues caused by LRU-K (known as the \emph{early page replacement} and \emph{correlated references}). Early page replacement occurs when a page has been accessed less than k times and thus has the largest backward distance at the time of choosing a replacement victim; the CRP method automatically retains this page if its last access time is within the time-out period. The problem of correlated references occurs when a page was accessed multiple times by the same query or process in a short span of time, the CRP method collapses all references to a single page within the time-out period into one reference.

The basic idea about basic LRU-1 algorithm is that a page that is referenced more recently has higher probability of occurring in close future. LRU-K, $K > 1$ algorithm is based on a different idea. It is assuming that every page has a certain frequency of occurrence and it's trying to estimate the frequency based on last $K$ occurrences. Thus, the workloads that LRU-K, $K > 1$ works best on are workloads where each page has an implicit frequency of occurrence. The basic hypothesis is that LRU-K will correctly estimate the frequency of occurrence for each page and keep the pages with highest frequencies in the buffer. LRU-1 does not have any notion of frequencies. If there is a page that is only rarely used and is references in the buffer management, LRU-1 will keep it in the buffer manager for next $B$ accesses, where $B$ is size of the buffer. LRU-K, $K > 1$ will, however, estimate that this page is rarely used and kick it out of the buffer as soon as possible.

Because of that, we expect the workloads where each page has a constant probability of occurring are the workloads where LRU-K will outperform LRU-1 the most. LRU-1 assumes that last $B$ page references influence the next page reference. For artificial statistic workloads without memory, this is not the case. Thus, LRU-1 should perform badly and LRU-K should perform much better on those workloads, since it's trying to learn that frequency of occurrence. 

Two statistical workloads with those properties that O'Neil et al. used to evaluated LRU-K algorithms are two pool and zipfian. Both of these workloads have no memory and the probability of next page reference is defined by the model and not by previous page references. Evaluating on those two workloads is a proxy to show that LRU-K does a good job of estimating reference probabilities based on observed page accesses. 

We recreate their experiments on two pool and zipfian distributions. We expect LRU-K, $K > 1$ to keep pages with greater frequency of occurrence in the buffer and kick those with smaller frequencies out. LRU-1, on the other hand, can not learn those frequencies and we expect it to lose a lot of buffer space keeping the pages with low probabilities, just because they were referenced recently. Thus, we expect LRU-K, $K > 1$ to outperform LRU-1 on two pool and zipfian distributions.

In the real database workload, however, the next page referenced indeed depends on previous referenced pages, especially within a transaction. The way LRU-K, $K > 1$ takes that into account is by Correlated Reference Period (CRP) discussed earlier.

As a third experiment, we used a trace from instrumented PostgreSQL running \texttt{pgbench}. Because of the mentioned effect, we expect to see worse LRU-K, $K > 1$ performance with disabled CRP. If CRP is disabled, LRU-K would just kick rarely occurring pages out of the buffer as soon as possible, although they will likely get referenced again by the same transaction. However, if we set CRP to a appropriate value, we expect to see LRU-K outperforming LRU-1. The reason for this is that in the database workload, some pages have higher frequencies of occurrence, just like in two pool and zipfian case. LRU-1 is not able to capture those frequencies, while LRU-K is.

We organize this paper as follows. In section \ref{sec:method} we discuss our experiment protocol and methodology. In section \ref{sec:results} we present our results and in section \ref{sec:conc} we conclude.